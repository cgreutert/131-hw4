---
title: "Homework Four: Resampling"
author: "Carly Greutert"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, include=FALSE}
library(tidymodels)
library(ggplot2)
library(discrim)
library(corrr)
library(klaR) # for naive bayes
library(caret)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(corrplot)
library(ggthemes)
library(cli)
library(recipes)
library(pROC)
library(yardstick)
library(MASS)
library(poissonreg)
library(naivebayes)
tidymodels_prefer()
```

```{r echo=TRUE, message=FALSE}
titanic <- read_csv('C:\\Program Files\\Git\\tmp\\131-hw4\\titanic.csv')
names <- c('pclass', 'survived')
titanic[,names] <- lapply(titanic[,names] , factor)
```

1.
```{r}
set.seed(777)
titanic_split <- initial_split(titanic, prop = 0.80, strata = 'survived')
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train)
titanic_recipe <- titanic_recipe %>%
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors())%>%
  step_interact(~starts_with("sex"):fare) %>%
  step_interact(~age:fare)
dim(titanic_train)
dim(titanic_test)
```
From the dim() function above, we can verify the number of observations was split correctly.                         
2.
```{r}
titanic_folds <- vfold_cv(titanic_train, v = 10)
titanic_folds
```

3. What we are doing above is resampling by splitting our training data into further subsets in order to provide a better evaluation for our model. K-fold cross-validation splits our data K-ways and has K-1 subsets and one subset for validation. It will help us train our model further. We should use k-fold cross-validation, rather than simply fitting and testing models on the entire training set in order to decide which model produces the lowest error rates and is therefore the best model to use. If we did use the entire training set, the resampling method used would be the validation set approach.                                                      
4. 
```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")
lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")
qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)
```
Across 10 folds, I will be fitting 30 models to the data.                                       
5.
```{r}
log_fit <- log_wkflow %>% fit_resamples(titanic_folds)
log_fit
lda_fit <- lda_wkflow %>% fit_resamples(titanic_folds)
lda_fit
qda_fit <- qda_wkflow %>% fit_resamples(titanic_folds)
qda_fit
```

6.
```{r}
collect_metrics(log_fit)
collect_metrics(lda_fit)
collect_metrics(qda_fit)
```
The means for accuracy and roc_auc all perform similarly across these three models in terms of the mean. Thus, standard error will be a more helpful in determining the best fitted model. Above, we see our QDA fitted model has a 0.018 standard error as opposed to the others 0.019 error, a small but noticeable difference. Therefore, QDA is our best fitted model since it has the lowest standard error.                                                                     
7.
```{r}
final_fit <- fit(qda_wkflow, titanic_train)
final_fit
```

8.
```{r}
predict(final_fit, new_data = titanic_test, type = "class") %>% 
  bind_cols(titanic_test %>% select(survived)) %>% 
  accuracy(truth = survived, estimate = .pred_class)
```
My model's testing accuracy ended up performing very similarly to the model on the folded data. This is to be expected since the folding of the data allowed for more training and testing so that it will perform similarly on new data, which it accomplished with roughly 82% accuracy. 